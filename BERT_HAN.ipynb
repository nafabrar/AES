{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "BERT_HAN.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nafabrar/AES/blob/master/BERT_HAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzGz9ReFcz5z",
        "colab_type": "text"
      },
      "source": [
        "## Notebook for training a BERT-HAN model\n",
        "\n",
        "This notebook can be used to train a HAN model with BERT sentence embedding as input. The input can be generated using BERT as service (https://github.com/hanxiao/bert-as-service). This notebook creates the BERT embedding from the TOEFL essay data after running TOEFL_dataParse.py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX9G_Zr7dRlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e03c32b5-d1b4-49f4-ce03-a5fe65c894e1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL3pDgGqefpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e442e0d9-6484-4958-a204-b91892f6d4ed"
      },
      "source": [
        "%cd /content/gdrive/My Drive/AES/AES"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/AES/AES\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcZy248rcz51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import io\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.set_printoptions(threshold = 10000)\n",
        "import collections\n",
        "import random\n",
        "\n",
        "from tensorflow.contrib.rnn import LSTMCell as Cell #for GRU: custom implementation with normalization\n",
        "from tensorflow.python.ops.rnn import dynamic_rnn as rnn\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
        "from tensorflow.contrib.rnn import DropoutWrapper\n",
        "\n",
        "from attention import attention as attention\n",
        "from ordloss import *\n",
        "from utils import *\n",
        "from dataUtils_mult import *\n",
        "\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bcXIUkfcz56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LENGTH_D = 25\n",
        "train_split = 0.9\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "# system parameters\n",
        "HIDDEN_SIZE_D = 150\n",
        "ATTENTION_SIZE_D = 50\n",
        "KEEP_PROB = 0.7\n",
        "DELTA = 0.75\n",
        "X_R = random.randint(1,10000)\n",
        "b_len = 768\n",
        "\n",
        "#read train and val data\n",
        "fpath = 'data/TOEFL/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JafjD8YJcz5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LEN_D = SEQUENCE_LENGTH_D\n",
        "df_train = pd.read_csv(os.path.join(fpath, 'train.csv'))\n",
        "\n",
        "text = df_train['text1']\n",
        "df_val = pd.read_csv(os.path.join(fpath,'test.csv'))\n",
        "\n",
        "text_val = df_val['text1']\n",
        "text_train = df_train['text1']\n",
        "rank_val = df_val['label']\n",
        "rank_train = df_train['label']\n",
        "\n",
        "target_val = np.array(rank_val)\n",
        "target_train = np.array(rank_train)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "integer_encoded = target_train.reshape(len(target_train), 1)\n",
        "y_train = onehot_encoder.fit_transform(integer_encoded)\n",
        "\n",
        "integer_encoded_val = target_val.reshape(len(target_val), 1)\n",
        "y_test = onehot_encoder.fit_transform(integer_encoded_val)\n",
        "\n",
        "\n",
        "### Get bert senence mebeddings for each sentence in an essay\n",
        "## Based on BERT serving client https://pypi.org/project/bert-serving-client/\n",
        "## base uncased model\n",
        "## Pooling = NONE for token level representation\n",
        "## bert-serving-start -model_dir /temp/uncased_L-12_H-768_A-12/ -num_worker=4 -max_seq_len=40\n",
        "from bert_serving.client import BertClient\n",
        "bc = BertClient()\n",
        "#b_len = 768\n",
        "X_train = []\n",
        "\n",
        "for i in df_train['text1']:\n",
        "    i = sent_tokenize(i)\n",
        "    X_train.extend(bc.encode(i[:SEQUENCE_LEN_D]))\n",
        "    for k in range(max(SEQUENCE_LEN_D -  (len(i)), 0)):\n",
        "        X_train.append([0]*b_len) # pad token maps to 0\n",
        "\n",
        "X_train = np.array(X_train)  \n",
        "\n",
        "X_test = []\n",
        "\n",
        "for i in df_val['text1']:\n",
        "    i = sent_tokenize(i)\n",
        "    X_test.extend(bc.encode(i[:SEQUENCE_LEN_D]))\n",
        "    for k in range(max(SEQUENCE_LEN_D -  (len(i)), 0)):\n",
        "        X_test.append([0]*b_len) # pad token maps to 0\n",
        "X_test = np.array(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAY7GOufcz6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_len = int(train_split*len(y_train))\n",
        "X_train, y_train, X_val, y_val  = X_train[:tr_len*SEQUENCE_LEN_D], y_train[:tr_len], X_train[tr_len*SEQUENCE_LEN_D:], y_train[tr_len:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnW-VZehcz6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = zero_pad_test(X_test, BATCH_SIZE*SEQUENCE_LENGTH_D)\n",
        "y_test = zero_pad_test(y_test, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ReIyY21cz6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(111)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XQ5FHJfcz6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Different placeholders\n",
        "num_classes = y_train.shape[1]\n",
        "batch_ph = tf.placeholder(tf.float32, [None, b_len])\n",
        "ind_list_ph = tf.placeholder(tf.int32, [None])\n",
        "target_ph = tf.placeholder(tf.float32, [None,num_classes])\n",
        "\n",
        "seq_len_ph_d = tf.placeholder(tf.int32, [None])\n",
        "keep_prob_ph = tf.placeholder(tf.float32)\n",
        "doc_size_ph = tf.placeholder(tf.int32,[None])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvznIg1ocz6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch = tf.reshape(batch_ph,[BATCH_SIZE, SEQUENCE_LENGTH_D, b_len])\n",
        "\n",
        "with tf.variable_scope('document'):\n",
        "    fw_cell_d = Cell(HIDDEN_SIZE_D)\n",
        "    bw_cell_d = Cell(HIDDEN_SIZE_D)\n",
        "    \n",
        "    fw_cell_d = DropoutWrapper(fw_cell_d, input_keep_prob=keep_prob_ph, \n",
        "                             output_keep_prob=keep_prob_ph,state_keep_prob=keep_prob_ph,\n",
        "                             variational_recurrent=True, input_size=batch.get_shape()[-1], \n",
        "                             dtype = tf.float32)\n",
        "    bw_cell_d = DropoutWrapper(bw_cell_d, input_keep_prob=keep_prob_ph, \n",
        "                             output_keep_prob=keep_prob_ph,state_keep_prob= keep_prob_ph,\n",
        "                             variational_recurrent=True, input_size=batch.get_shape()[-1], \n",
        "                             dtype = tf.float32)\n",
        "    \n",
        "    rnn_outputs_d, _ = bi_rnn(fw_cell_d, bw_cell_d, inputs=batch, \n",
        "                              sequence_length=seq_len_ph_d, dtype=tf.float32)\n",
        "    attention_output_d, alphas_d = attention(rnn_outputs_d, ATTENTION_SIZE_D, seq_len_ph_d, return_alphas=True)\n",
        "\n",
        "# Dropout\n",
        "drop = tf.nn.dropout(attention_output_d, keep_prob_ph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMPpqmBAcz6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ordinal = True\n",
        "if ordinal:\n",
        "    # For ordinal regression, same weights for each class\n",
        "    W = tf.Variable(tf.truncated_normal([drop.get_shape()[1].value], stddev=0.1))\n",
        "    W_ = tf.transpose(tf.reshape(tf.tile(W,[num_classes - 1]),[num_classes - 1, drop.get_shape()[1].value]))\n",
        "    b = tf.Variable(tf.cast(tf.range(num_classes - 1), dtype = tf.float32))\n",
        "    y_hat_ = tf.nn.xw_plus_b(drop, tf.negative(W_), b)\n",
        "\n",
        "    # Predicted labels and logits\n",
        "    y_preds, logits = preds(y_hat_,BATCH_SIZE)\n",
        "    y_true = tf.argmax(target_ph, axis = 1)\n",
        "\n",
        "    # Ordinal loss\n",
        "    loss = ordloss_m(y_hat_, target_ph, BATCH_SIZE)\n",
        "    c = stats.spearmanr\n",
        "    str_score = \"Spearman rank:\"\n",
        "    \n",
        "# Calculate and clip gradients\n",
        "max_gradient_norm = 5\n",
        "lr = 1e-4\n",
        "params = tf.trainable_variables()\n",
        "gradients = tf.gradients(loss, params)\n",
        "clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
        "optimizer_ = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "optimizer = optimizer_.apply_gradients(\n",
        "    zip(clipped_gradients, params))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH3gDa8ocz6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUSH-lbVcz6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE, seq_len = SEQUENCE_LENGTH_D)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk6L88QJcz6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_counter = 0\n",
        "val_counter = []\n",
        "config = tf.ConfigProto(inter_op_parallelism_threads=24,\n",
        "                        intra_op_parallelism_threads=24)\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config = config)\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL9rcQxxcz6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Main training task\n",
        "\n",
        "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE, seq_len = SEQUENCE_LENGTH_D)\n",
        "val_batch_generator = batch_generator(X_val, y_val, BATCH_SIZE, seq_len = SEQUENCE_LENGTH_D)\n",
        "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE, seq_len = SEQUENCE_LENGTH_D, shuffle = False)\n",
        "\n",
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "val_counter = []\n",
        "val_count = 50\n",
        "NUM_EPOCHS = 50\n",
        "doc_size_np = np.array([0]*SEQUENCE_LENGTH_D)\n",
        "batch_counter = 0\n",
        "\n",
        "loss_train = 0\n",
        "\n",
        "print('Training on TOEFL data')\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
        "\n",
        "    # Training\n",
        "    num_batches = X_train.shape[0] // (BATCH_SIZE*SEQUENCE_LENGTH_D)\n",
        "    true = []\n",
        "    ypreds = []\n",
        "\n",
        "    for bx in range(num_batches):\n",
        "        batch_counter += 1\n",
        "        x_batch, y_batch = next(train_batch_generator)\n",
        "        seq_len_d = []               \n",
        "        l = SEQUENCE_LENGTH_D\n",
        "        for i in range(0,len(x_batch),l):\n",
        "            for j in range(i,i+l):\n",
        "                if 0 in x_batch[j]:\n",
        "                    if list(x_batch[j]).index(0) == 0:\n",
        "                        seq_len_d.append(j%l)\n",
        "                        break\n",
        "                elif j == i+l-1:\n",
        "                    seq_len_d.append(l)\n",
        "\n",
        "        seq_len_d = np.array(seq_len_d)\n",
        "\n",
        "        y_preds_, loss_tr,  _  = sess.run([y_preds, loss,  optimizer],\n",
        "                                   feed_dict={batch_ph: x_batch,\n",
        "                                              target_ph: y_batch,\n",
        "                                              seq_len_ph_d: seq_len_d,\n",
        "                                              doc_size_ph: doc_size_np,\n",
        "                                              keep_prob_ph: KEEP_PROB})\n",
        "        loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
        "        ypreds.extend(y_preds_)\n",
        "        t = np.argmax(y_batch, axis = 1)\n",
        "        true.extend(t)\n",
        "\n",
        "        sp = c(y_preds_,t)\n",
        "        if ordinal: \n",
        "            sp = sp[0]\n",
        "        train_accuracy.append(sp)\n",
        "        \n",
        "        \n",
        "        #testing on the validation set            \n",
        "        if batch_counter%val_count == 0:\n",
        "            val_counter.append(batch_counter)\n",
        "            x_batch, y_batch = next(val_batch_generator)\n",
        "            seq_len_d = []               \n",
        "            l = SEQUENCE_LENGTH_D\n",
        "            for i in range(0,len(x_batch),l):\n",
        "                for j in range(i,i+l):\n",
        "                    if 0 in x_batch[j]:\n",
        "                        if list(x_batch[j]).index(0) == 0:\n",
        "                            seq_len_d.append(j%l)\n",
        "                            break\n",
        "                    elif j == i+l-1:\n",
        "                        seq_len_d.append(l)\n",
        "\n",
        "            seq_len_d = np.array(seq_len_d)\n",
        "\n",
        "            y_preds_,loss_t = sess.run([y_preds,loss],\n",
        "                          feed_dict={batch_ph: x_batch,\n",
        "                                target_ph: y_batch,\n",
        "                                seq_len_ph_d: seq_len_d,\n",
        "                                doc_size_ph: doc_size_np,\n",
        "                                keep_prob_ph: 1.0})\n",
        "            ypreds.extend(y_preds_)\n",
        "            t = np.argmax(y_batch, axis = 1)\n",
        "            true.extend(t)\n",
        "\n",
        "            sp = c(y_preds_,t)\n",
        "            if ordinal: \n",
        "                sp = sp[0]\n",
        "            val_accuracy.append(sp)\n",
        "            #saver.save(sess, MODEL_PATH, global_step = batch_counter)\n",
        "\n",
        "    print('training loss: ' + str(loss_train))\n",
        "    spr = c(true, ypreds)\n",
        "    if ordinal:\n",
        "        spr = spr[0]\n",
        "    print('Training '+ str_score + str(spr))\n",
        "    print('Val ' + str(np.mean(val_accuracy)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUaNnRXwcz6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#testing on the test set\n",
        "num_batches = X_test.shape[0] // (BATCH_SIZE*SEQUENCE_LENGTH_D)\n",
        "true = []\n",
        "ypreds = []\n",
        "\n",
        "for bx in range(num_batches):\n",
        "    x_batch, y_batch = next(test_batch_generator)\n",
        "    seq_len_d = []               \n",
        "    l = SEQUENCE_LENGTH_D\n",
        "    for i in range(0,len(x_batch),l):\n",
        "        for j in range(i,i+l):\n",
        "            if 0 in x_batch[j]:\n",
        "                if list(x_batch[j]).index(0) == 0:\n",
        "                    seq_len_d.append(j%l)\n",
        "                    break\n",
        "            elif j == i+l-1:\n",
        "                seq_len_d.append(l)\n",
        "\n",
        "    seq_len_d = np.array(seq_len_d)\n",
        "\n",
        "    y_preds_= sess.run([y_preds],\n",
        "                  feed_dict={batch_ph: x_batch,\n",
        "                        target_ph: y_batch,\n",
        "                        seq_len_ph_d: seq_len_d,\n",
        "                        doc_size_ph: doc_size_np,\n",
        "                        keep_prob_ph: 1.0})\n",
        "    ypreds.extend(y_preds_)\n",
        "    t = np.argmax(y_batch, axis = 1)\n",
        "    true.extend(t)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6K9Hiarcz6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#len(ypreds[0])\n",
        "ypreds = [j for sub in ypreds for j in sub]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTyGqhSDcz6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_len = len(df_val)\n",
        "true = true[:y_test_len]\n",
        "ypreds = ypreds[:y_test_len]\n",
        "\n",
        "spr = c(true, ypreds)\n",
        "\n",
        "if ordinal:\n",
        "    spr = spr[0]\n",
        "print('Test set '+ str_score + str(spr))\n",
        "\n",
        "rank = stats.spearmanr\n",
        "print('sp rho')\n",
        "print(rank(true, ypreds))\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score as kappa\n",
        "print('qwk')\n",
        "print(kappa(true, ypreds, weights=\"quadratic\"))\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "print('pearson')\n",
        "print(pearsonr(true,ypreds))\n",
        "\n",
        "print('kappa')\n",
        "print(kappa(true, ypreds, weights=None))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}