{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "ASAP_DM_BCA_train.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nafabrar/AES/blob/master/ASAP_DM_BCA_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz5EYUm8PDz5",
        "colab_type": "text"
      },
      "source": [
        "## Notebook for training a pretrained DM-BCA model\n",
        "\n",
        "The model stored in bca_dm_model is pretrained on the discourse marker prediction task. This notebook trains the model on ASAP essay data. Please pre-process the ASAP data using the script ASAP_dataParse.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1bUuTRuQYXf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "91c80348-bba2-42f0-beb0-b79abea228a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My Drive/AES/AES"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/My Drive/AES/AES\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLhGsjrzbzAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "9ee03b89-e7ab-43ef-f1ca-6b5697d0699a"
      },
      "source": [
        "!pip install tensorflow==1.12\n",
        "!pip install keras-attention\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.12\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/cc/ca70b78087015d21c5f3f93694107f34ebccb3be9624385a911d4b52ecef/tensorflow-1.12.0-cp36-cp36m-manylinux1_x86_64.whl (83.1MB)\n",
            "\u001b[K     |████████████████████████████████| 83.1MB 41kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (3.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (0.9.0)\n",
            "Collecting tensorboard<1.13.0,>=1.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/53/8d32ce9471c18f8d99028b7cef2e5b39ea8765bd7ef250ca05b490880971/tensorboard-1.12.2-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 33.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.12) (1.29.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.12) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.12) (47.3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.13.0,>=1.12.0->tensorflow==1.12) (3.1.0)\n",
            "Installing collected packages: tensorboard, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "Successfully installed tensorboard-1.12.2 tensorflow-1.12.0\n",
            "Collecting keras-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/9a/e9/1373dff1b2dd7cb45208d224c84d7a2bddcf0519f01d98f07d8ce15dc4be/keras_attention-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-attention) (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-attention) (1.0.8)\n",
            "Installing collected packages: keras-attention\n",
            "Successfully installed keras-attention-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q23rmLTzv0PC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e7f7b465-faa2-44ae-f2d3-2bf6212a0941"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXRJ4ILjPDz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import io\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.set_printoptions(threshold = 10000)\n",
        "import collections\n",
        "import random\n",
        "\n",
        "from tensorflow.contrib.rnn import LSTMCell as Cell #for GRU: custom implementation with normalization\n",
        "from tensorflow.python.ops.rnn import dynamic_rnn as rnn\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
        "from tensorflow.contrib.rnn import DropoutWrapper\n",
        "\n",
        "from attention import attention as attention\n",
        "from bca_ import *\n",
        "from ordloss import *\n",
        "from utils import *\n",
        "from datautilsbca import *\n",
        "\n",
        "\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZyxva2DPDz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read data; SEQUENCE_LENGTH is maximum length of sentence in words, SEQUENCE_LENGTH_D is maximum length of document in sentences. \n",
        "SEQUENCE_LENGTH = 40\n",
        "SEQUENCE_LENGTH_D = 25\n",
        "max_vocab = 75000\n",
        "train_split = 0.9\n",
        "BATCH_SIZE = 20\n",
        "\n",
        "# system parameters\n",
        "HIDDEN_SIZE = 150\n",
        "HIDDEN_SIZE_D = 150\n",
        "ATTENTION_SIZE = 75\n",
        "ATTENTION_SIZE_D = 50\n",
        "LAYER_1 = 500\n",
        "LAYER_2 = 250\n",
        "LAYER_3 = 100\n",
        "KEEP_PROB = 0.7\n",
        "#NUM_EPOCHS = 1  # max val_acc at __\n",
        "DELTA = 0.75"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6ikNe_JPD0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the path specifies which set and which fold will be used\n",
        "fpath = 'data/ASAP/fold_0/1'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOrnWej6PD0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#add dict name\n",
        "dict_name = 'bca_dm_model/dict.csv'\n",
        "# load the dictionary from the pre-trained model folder\n",
        "import csv \n",
        "dictionary = {}\n",
        "for key,val in csv.reader(open(dict_name)):\n",
        "    dictionary[key] = val\n",
        "\n",
        "# the test data set; the fformat is csv, with the text column labelled 'text'\n",
        "df_test = pd.read_csv(os.path.join(fpath,'test.csv'))\n",
        "df_train = pd.read_csv(os.path.join(fpath,'train.csv'))\n",
        "df_val = pd.read_csv(os.path.join(fpath,'dev.csv'))\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xv7jE2XPD0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_test_set(df_test, dictionary, SEQUENCE_LEN_D = 40, SEQUENCE_LEN = 65, BATCH_SIZE = 10, min_= 0, max_ = 10):\n",
        "    count_oov_train = 0\n",
        "    count_iv_train = 0\n",
        "    X_train = []\n",
        "\n",
        "    for i in df_test['text1']:\n",
        "        i = sent_tokenize(i)\n",
        "        X_train.append([dictionary['START_SENT']])\n",
        "        for j in i[:SEQUENCE_LEN_D-2]:\n",
        "            j = str(j)\n",
        "            #print(j)\n",
        "            x = j.split()\n",
        "            data = []\n",
        "            #print(x)\n",
        "            data.append(dictionary['START'])\n",
        "            for word in x:\n",
        "                if word in dictionary:\n",
        "                    index = dictionary[word]\n",
        "                    count_iv_train += 1\n",
        "\n",
        "                else:\n",
        "                    index = dictionary['UNK']\n",
        "                    count_oov_train += 1\n",
        "                data.append(index)\n",
        "            data.append(dictionary['END'])\n",
        "            X_train.append(data)\n",
        "        X_train.append([dictionary['END_SENT']])\n",
        "        for k in range(max(SEQUENCE_LEN_D -  (len(i)+2), 0)):\n",
        "            X_train.append([0]) # pad token maps to 0\n",
        "\n",
        "    print('len of test set: ', len(X_train)//BATCH_SIZE)\n",
        "\n",
        "        \n",
        "    rank_val = list(df_test['label'].values)\n",
        "    rank_val.extend([i for i in range(min_,max_)])\n",
        "    target_val = np.array(rank_val)\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\n",
        "    \n",
        "    integer_encoded = target_val.reshape(len(target_val), 1)\n",
        "    y_test = onehot_encoder.fit_transform(integer_encoded)\n",
        "    y_test = y_test[:-len([i for i in range(min_,max_)])]\n",
        "\n",
        "    return X_train, y_test"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dLEf_vkPD0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ad60cf6d-e6f3-4398-a661-7e9c36e2b422"
      },
      "source": [
        "#min and max+1 score range for the ASAP set; this is to ensure that all labels are present in the one-hot encoding\n",
        "mi = 2\n",
        "ma = 13\n",
        "\n",
        "X_test, y_test = read_test_set(df_test, dictionary, SEQUENCE_LEN_D = SEQUENCE_LENGTH_D, SEQUENCE_LEN = SEQUENCE_LENGTH, min_= mi, max_ = ma)\n",
        "X_val, y_val = read_test_set(df_val, dictionary, SEQUENCE_LEN_D = SEQUENCE_LENGTH_D, SEQUENCE_LEN = SEQUENCE_LENGTH, min_= mi, max_ = ma)\n",
        "X_train, y_train = read_test_set(df_train, dictionary, SEQUENCE_LEN_D = SEQUENCE_LENGTH_D, SEQUENCE_LEN = SEQUENCE_LENGTH, min_= mi, max_ = ma)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of test set:  892\n",
            "len of test set:  890\n",
            "len of test set:  2675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QF6ntZKBPD0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e76eb1f1-91f0-4613-9adb-5cbfeb5d5a41"
      },
      "source": [
        "doc_vocab_size = len(dictionary)\n",
        "NUM_WORDS = doc_vocab_size\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "\n",
        "print('Sentence length:',SEQUENCE_LENGTH)\n",
        "print('Document length:',SEQUENCE_LENGTH_D)\n",
        "\n",
        "print('Sentence length:',SEQUENCE_LENGTH)\n",
        "print('Document length:',SEQUENCE_LENGTH_D)\n",
        "print('Hidden size:',HIDDEN_SIZE)\n",
        "print('Hidden size sentence level:',HIDDEN_SIZE_D)\n",
        "\n",
        "y_test_len = len(y_test)\n",
        "\n",
        "#use ordinal regression; logistic regression if False\n",
        "ordinal = True"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence length: 40\n",
            "Document length: 25\n",
            "Sentence length: 40\n",
            "Document length: 25\n",
            "Hidden size: 150\n",
            "Hidden size sentence level: 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-tVnuOJPD0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sequences preprocessing\n",
        "vocabulary_size = doc_vocab_size \n",
        "\n",
        "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
        "X_test = zero_pad(X_test, SEQUENCE_LENGTH)\n",
        "X_val = zero_pad(X_val, SEQUENCE_LENGTH)\n",
        "\n",
        "#batch size padding \n",
        "X_test = zero_pad_test(X_test, BATCH_SIZE*SEQUENCE_LENGTH_D)\n",
        "y_test = zero_pad_test(y_test, BATCH_SIZE)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kt2gw_YPD0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "ffd29cb0-6a70-4e77-b4a4-311f1d053588"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "#Different placeholders\n",
        "num_classes_asap = y_train.shape[1]\n",
        "num_classes = 3\n",
        "num_classes_s = 8\n",
        "num_classes_s1 = 4\n",
        "batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH])\n",
        "ind_list_ph = tf.placeholder(tf.int32, [None])\n",
        "target_ph = tf.placeholder(tf.float32, [None,num_classes])\n",
        "target_ph_s = tf.placeholder(tf.float32, [None,num_classes_s])\n",
        "target_ph_s1 = tf.placeholder(tf.float32, [None,num_classes_s1])\n",
        "\n",
        "seq_len_ph = tf.placeholder(tf.int32, [None])\n",
        "seq_len_ph_d = tf.placeholder(tf.int32, [None])\n",
        "keep_prob_ph = tf.placeholder(tf.float32)\n",
        "doc_size_ph = tf.placeholder(tf.int32,[None])\n",
        "\n",
        "\n",
        "# Embedding layer\n",
        "embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
        "batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)\n",
        "batch_embedded = tf.nn.dropout(batch_embedded, keep_prob_ph)\n",
        "\n",
        "W_omega = tf.Variable(tf.random_uniform([HIDDEN_SIZE*2, HIDDEN_SIZE*2], -1.0, 1.0))\n",
        "# (Bi-)RNN layer(-s)\n",
        "with tf.variable_scope('sentence'):\n",
        "    fw_cell = Cell(HIDDEN_SIZE)\n",
        "    bw_cell = Cell(HIDDEN_SIZE)\n",
        "    \n",
        "    fw_cell = DropoutWrapper(fw_cell, input_keep_prob=keep_prob_ph, \n",
        "                             output_keep_prob=keep_prob_ph,state_keep_prob=keep_prob_ph,\n",
        "                             variational_recurrent=True, input_size=batch_embedded.get_shape()[-1], \n",
        "                             dtype = tf.float32)\n",
        "    bw_cell = DropoutWrapper(bw_cell, input_keep_prob=keep_prob_ph, \n",
        "                             output_keep_prob=keep_prob_ph,state_keep_prob= keep_prob_ph,\n",
        "                             variational_recurrent=True, input_size=batch_embedded.get_shape()[-1], \n",
        "                             dtype = tf.float32)\n",
        "    rnn_output, _ = bi_rnn(fw_cell, bw_cell, inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
        "\n",
        "    rnn_outputs = cross_attention(rnn_output, 2 , seq_len_ph, BATCH_SIZE, W_omega, time_major=False, return_alphas=False)\n",
        "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, seq_len_ph, return_alphas=True)\n",
        "    rnn_outputs_ = cross_attention(rnn_output, SEQUENCE_LENGTH_D, seq_len_ph, BATCH_SIZE, W_omega)\n",
        "    attention_output_, alphas_ = attention(rnn_outputs_ , ATTENTION_SIZE, seq_len_ph, return_alphas = True)\n",
        "    attention_output_ = tf.reshape(attention_output_,[BATCH_SIZE, -1, HIDDEN_SIZE*2*3])\n",
        "    \n",
        "with tf.variable_scope('document'):\n",
        "    fw_cell_d = Cell(HIDDEN_SIZE_D)\n",
        "    bw_cell_d = Cell(HIDDEN_SIZE_D)\n",
        "    \n",
        "    fw_cell_d = DropoutWrapper(fw_cell_d, input_keep_prob=keep_prob_ph, \n",
        "                             output_keep_prob=keep_prob_ph,state_keep_prob=keep_prob_ph,\n",
        "                             variational_recurrent=True, input_size=attention_output_.get_shape()[-1], \n",
        "                             dtype = tf.float32)\n",
        "    bw_cell_d = DropoutWrapper(bw_cell_d, input_keep_prob=keep_prob_ph, \n",
        "                             output_keep_prob=keep_prob_ph,state_keep_prob= keep_prob_ph,\n",
        "                             variational_recurrent=True, input_size=attention_output_.get_shape()[-1], \n",
        "                             dtype = tf.float32)\n",
        "    rnn_outputs_d, _ = bi_rnn(fw_cell_d, bw_cell_d, inputs=attention_output_, \n",
        "                              sequence_length=seq_len_ph_d, dtype=tf.float32)\n",
        "    \n",
        "    #rnn_outputs_d, _ = bi_rnn(Cell(HIDDEN_SIZE_D), Cell(HIDDEN_SIZE_D), inputs=attention_output, sequence_length=seq_len_ph_d, dtype=tf.float32)\n",
        "    attention_output_d, alphas_d = attention(rnn_outputs_d, ATTENTION_SIZE_D, seq_len_ph_d, return_alphas=True)\n",
        "\n",
        "# Dropout\n",
        "drop = tf.nn.dropout(attention_output_d, keep_prob_ph)\n",
        "\n",
        "\n",
        "\n",
        "#first classifier for first task using the representation from attention_outputs\n",
        "#adding more layers... \n",
        "attention_output_sentorder = tf.reshape(attention_output, [-1,HIDDEN_SIZE*2*2*3])\n",
        "W_s1_ = tf.Variable(tf.truncated_normal([HIDDEN_SIZE*2*2*3, LAYER_1], stddev=0.1))  \n",
        "b_s1_ = tf.Variable(tf.truncated_normal([LAYER_1]))\n",
        "y_hat_s1_ = tf.nn.xw_plus_b(attention_output_sentorder, W_s1_, b_s1_)\n",
        "W_s2 = tf.Variable(tf.truncated_normal([LAYER_1, LAYER_2], stddev=0.1))  \n",
        "b_s2 = tf.Variable(tf.truncated_normal([LAYER_2]))\n",
        "y_hat_s2 = tf.nn.xw_plus_b(y_hat_s1_, W_s2, b_s2)\n",
        "\n",
        "W_s = tf.Variable(tf.truncated_normal([LAYER_2, num_classes_s], stddev=0.1))  \n",
        "b_s = tf.Variable(tf.truncated_normal([num_classes_s]))\n",
        "y_hat_s = tf.nn.xw_plus_b(y_hat_s2, W_s, b_s)\n",
        "y_preds_s = tf.argmax(y_hat_s, axis = 1)\n",
        "loss_s = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat_s, labels=target_ph_s))\n",
        "\n",
        "#second classifier for second task using the representation from attention_outputs\n",
        "W_s1__ = tf.Variable(tf.truncated_normal([HIDDEN_SIZE*2*2*3, LAYER_1], stddev=0.1))  \n",
        "b_s1__ = tf.Variable(tf.truncated_normal([LAYER_1]))\n",
        "y_hat_s1__ = tf.nn.xw_plus_b(attention_output_sentorder, W_s1__, b_s1__)\n",
        "W_s2_ = tf.Variable(tf.truncated_normal([LAYER_1, LAYER_2], stddev=0.1))  \n",
        "b_s2_ = tf.Variable(tf.truncated_normal([LAYER_2]))\n",
        "y_hat_s2_ = tf.nn.xw_plus_b(y_hat_s1__, W_s2_, b_s2_)\n",
        "\n",
        "W_s1 = tf.Variable(tf.truncated_normal([LAYER_2, num_classes_s1], stddev=0.1))  \n",
        "b_s1 = tf.Variable(tf.truncated_normal([num_classes_s1]))\n",
        "y_hat_s1 = tf.nn.xw_plus_b(y_hat_s2_, W_s1, b_s1)\n",
        "y_preds_s1 = tf.argmax(y_hat_s1, axis = 1)\n",
        "loss_s1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat_s1, labels=target_ph_s1))\n",
        "\n",
        "\n",
        "if ordinal:\n",
        "    # For ordinal regression, same weights for each class\n",
        "    W = tf.Variable(tf.truncated_normal([drop.get_shape()[1].value], stddev=0.1))\n",
        "    W_ = tf.transpose(tf.reshape(tf.tile(W,[num_classes - 1]),[num_classes - 1, drop.get_shape()[1].value]))\n",
        "    b = tf.Variable(tf.cast(tf.range(num_classes - 1), dtype = tf.float32))\n",
        "    y_hat_ = tf.nn.xw_plus_b(drop, tf.negative(W_), b)\n",
        "\n",
        "    # Predicted labels and logits\n",
        "    y_preds, logits = preds(y_hat_,BATCH_SIZE)\n",
        "    y_true = tf.argmax(target_ph, axis = 1)\n",
        "\n",
        "    # Ordinal loss\n",
        "    loss = ordloss_m(y_hat_, target_ph, BATCH_SIZE)\n",
        "    c = stats.spearmanr\n",
        "    str_score = \"Spearman rank:\"\n",
        "    \n",
        "    \n",
        "else:\n",
        "    W = tf.Variable(tf.truncated_normal([drop.get_shape()[1].value, num_classes], stddev=0.1))  \n",
        "    b = tf.Variable(tf.truncated_normal([num_classes]))\n",
        "    y_hat_ = tf.nn.xw_plus_b(drop, W, b)\n",
        "    # Cross-entropy loss and optimizer initialization\n",
        "    y_preds = tf.argmax(y_hat_, axis = 1)\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat_, labels=target_ph))\n",
        "    c = accuracy_score\n",
        "    str_score = \"accucary:\"\n",
        "    \n",
        "# Calculate and clip gradients\n",
        "max_gradient_norm = 5\n",
        "lr = 5e-4\n",
        "params = tf.trainable_variables()\n",
        "gradients = tf.gradients(loss, params)\n",
        "clipped_gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n",
        "optimizer_ = tf.train.AdamOptimizer(learning_rate=lr)\n",
        "optimizer = optimizer_.apply_gradients(\n",
        "    zip(clipped_gradients, params))\n",
        "\n",
        "#second optimizer for sentence order\n",
        "gradients_s = tf.gradients(loss_s, params)\n",
        "clipped_gradients_s, _ = tf.clip_by_global_norm(gradients_s, max_gradient_norm)\n",
        "optimizer_s = optimizer_.apply_gradients(\n",
        "    zip(clipped_gradients_s, params))\n",
        "\n",
        "#third optimizer for sentence order\n",
        "gradients_s1 = tf.gradients(loss_s1, params)\n",
        "clipped_gradients_s1, _ = tf.clip_by_global_norm(gradients_s1, max_gradient_norm)\n",
        "optimizer_s1 = optimizer_.apply_gradients(\n",
        "    zip(clipped_gradients_s1, params))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-16-24224c31a52c>:83: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjkEXmblPD0R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = 'bca_dm_model/model300-20500-1800-1800'\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBfQAUR9PD0T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0634b112-bd4f-4beb-d2b8-327a3c4cbc02"
      },
      "source": [
        "sess = tf.Session()\n",
        "saver.restore(sess, MODEL_PATH)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from bca_dm_model/model300-20500-1800-1800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBqvRjLpPD0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###ASAP \n",
        "target_ph_asap = tf.placeholder(tf.float32, [None,num_classes_asap])\n",
        "\n",
        "W_asap = tf.Variable(tf.truncated_normal([drop.get_shape()[1].value], stddev=0.1))\n",
        "W_asap_ = tf.transpose(tf.reshape(tf.tile(W_asap,[num_classes_asap - 1]),[num_classes_asap - 1, drop.get_shape()[1].value]))\n",
        "b_asap = tf.Variable(tf.cast(tf.range(num_classes_asap - 1), dtype = tf.float32))\n",
        "y_hat_asap_ = tf.nn.xw_plus_b(drop, tf.negative(W_asap_), b_asap)\n",
        "\n",
        "# Predicted labels and logits\n",
        "y_preds_asap, logits_asap = preds(y_hat_asap_,BATCH_SIZE)\n",
        "y_true_asap = tf.argmax(target_ph_asap, axis = 1)\n",
        "\n",
        "# Ordinal loss\n",
        "loss_asap = ordloss_m(y_hat_asap_, target_ph_asap, BATCH_SIZE)\n",
        "\n",
        "gradients_asap = tf.gradients(loss_asap, params)\n",
        "clipped_gradients_asap, _ = tf.clip_by_global_norm(gradients_asap, max_gradient_norm)\n",
        "optimizer_asap = optimizer_.apply_gradients(\n",
        "    zip(clipped_gradients_asap, params))\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwi5yQiOPD0Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "d759cfcd-e3c7-4571-af0b-921bce4b59f0"
      },
      "source": [
        "uninitialized_vars = []\n",
        "for var in tf.all_variables():\n",
        "    try:\n",
        "        sess.run(var)\n",
        "    except tf.errors.FailedPreconditionError:\n",
        "        uninitialized_vars.append(var)\n",
        "\n",
        "init_new_vars_op = tf.initialize_variables(uninitialized_vars)\n",
        "sess.run(init_new_vars_op)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-20-2417e917cc95>:2: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Please use tf.global_variables instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py:189: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.variables_initializer` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoygY-oZPD0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Main training task\n",
        "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE, seq_len = SEQUENCE_LENGTH_D)\n",
        "val_batch_generator = batch_generator(X_val, y_val, BATCH_SIZE, seq_len = SEQUENCE_LENGTH_D)\n",
        "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE, seq_len = SEQUENCE_LENGTH_D, shuffle = False)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2xXjkX-PD0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "outputId": "7c732e07-9724-4243-e51d-f84855c92cbe"
      },
      "source": [
        "train_accuracy = []\n",
        "val_accuracy = []\n",
        "val_counter = []\n",
        "val_count = 50\n",
        "loss_train = 0\n",
        "NUM_EPOCHS = 25\n",
        "doc_size_np = np.array([0]*SEQUENCE_LENGTH_D)\n",
        "batch_counter = 0\n",
        "KEEP_PROB = 0.75\n",
        "print('Training on ASAP data')\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
        "\n",
        "    # Training\n",
        "    num_batches = X_train.shape[0] // (BATCH_SIZE*SEQUENCE_LENGTH_D)\n",
        "    true = []\n",
        "    ypreds = []\n",
        "    #y_temp = np.zeros((BATCH_SIZE,num_classes))\n",
        "\n",
        "    for bx in range(num_batches):\n",
        "        batch_counter += 1\n",
        "        x_batch, y_batch = next(train_batch_generator)\n",
        "        seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
        "        seq_len_d = []               \n",
        "        l = SEQUENCE_LENGTH_D\n",
        "        for i in range(0,len(x_batch),l):\n",
        "            for j in range(i,i+l):\n",
        "                if list(x_batch[j]).index(0) == 0:\n",
        "                    seq_len_d.append(j%l)\n",
        "                    break\n",
        "                elif j == i+l-1:\n",
        "                    seq_len_d.append(l)\n",
        "\n",
        "        seq_len_d = np.array(seq_len_d)\n",
        "\n",
        "        y_preds_, loss_tr,  _  = sess.run([y_preds_asap, loss_asap,  optimizer_asap],\n",
        "                                   feed_dict={batch_ph: x_batch,\n",
        "                                              target_ph_asap: y_batch,\n",
        "                                              seq_len_ph: seq_len,\n",
        "                                              seq_len_ph_d: seq_len_d,\n",
        "                                              doc_size_ph: doc_size_np,\n",
        "                                              keep_prob_ph: KEEP_PROB})\n",
        "        loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
        "        ypreds.extend(y_preds_)\n",
        "        t = np.argmax(y_batch, axis = 1)\n",
        "        true.extend(t)\n",
        "\n",
        "        sp = c(y_preds_,t)\n",
        "        if ordinal: \n",
        "            sp = sp[0]\n",
        "        train_accuracy.append(sp)\n",
        "\n",
        "        #testing on the validation set            \n",
        "        if batch_counter%val_count == 0:\n",
        "            val_counter.append(batch_counter)\n",
        "            x_batch, y_batch = next(val_batch_generator)\n",
        "            seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
        "            seq_len_d = []               \n",
        "            l = SEQUENCE_LENGTH_D\n",
        "            for i in range(0,len(x_batch),l):\n",
        "                for j in range(i,i+l):\n",
        "                    if list(x_batch[j]).index(0) == 0:\n",
        "                        seq_len_d.append(j%l)\n",
        "                        break\n",
        "                    elif j == i+l-1:\n",
        "                        seq_len_d.append(l)\n",
        "\n",
        "            seq_len_d = np.array(seq_len_d)\n",
        "\n",
        "            y_preds_,loss_t,att_test = sess.run([y_preds_asap,loss_asap,attention_output_],\n",
        "                          feed_dict={batch_ph: x_batch,\n",
        "                                target_ph_asap: y_batch,\n",
        "                                seq_len_ph: seq_len,\n",
        "                                seq_len_ph_d: seq_len_d,\n",
        "                                doc_size_ph: doc_size_np,\n",
        "                                keep_prob_ph: 1.0})\n",
        "            ypreds.extend(y_preds_)\n",
        "            t = np.argmax(y_batch, axis = 1)\n",
        "            true.extend(t)\n",
        "\n",
        "            sp = c(y_preds_,t)\n",
        "            if ordinal: \n",
        "                sp = sp[0]\n",
        "            val_accuracy.append(sp)\n",
        "    print('training loss: ' + str(loss_train))\n",
        "    spr = c(true, ypreds)\n",
        "    if ordinal:\n",
        "        spr = spr[0]\n",
        "    print('Training '+ str_score + str(spr))\n",
        "    print('Val ' + str(np.mean(val_accuracy)))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on ASAP data\n",
            "epoch: 0\t"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
            "  return (a < x) & (x < b)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
            "  return (a < x) & (x < b)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
            "  cond2 = cond0 & (x <= _a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss: 1.6957044600683702\n",
            "Training Spearman rank:0.16855440700903063\n",
            "Val 0.8059974857283168\n",
            "epoch: 1\ttraining loss: 1.6240664957238848\n",
            "Training Spearman rank:0.555913659063807\n",
            "Val 0.837723444886757\n",
            "epoch: 2\t"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-e2c4fa5a8739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m                                               \u001b[0mseq_len_ph_d\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_len_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                                               \u001b[0mdoc_size_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc_size_np\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                                               keep_prob_ph: KEEP_PROB})\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mDELTA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mDELTA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mypreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_preds_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVfMR8bnPD0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ade23a71-59f4-40f5-8fde-03848d9ed14c"
      },
      "source": [
        "df_test = pd.read_csv(os.path.join(fpath,'test.csv'))\n",
        "X_test, y_test = read_test_set(df_test, dictionary, SEQUENCE_LEN_D = SEQUENCE_LENGTH_D, SEQUENCE_LEN = SEQUENCE_LENGTH, min_= mi, max_ = ma)\n",
        "X_test = zero_pad(X_test, SEQUENCE_LENGTH)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len of test set:  892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7kVRP-yPD0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batch size padding \n",
        "X_test = zero_pad_test(X_test, BATCH_SIZE*SEQUENCE_LENGTH_D)\n",
        "y_test = zero_pad_test(y_test, BATCH_SIZE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHJxD-QDPD0k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8f8a7637-e454-4a49-f245-ca24a19e5046"
      },
      "source": [
        "#testing on the test set\n",
        "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE, seq_len = SEQUENCE_LENGTH_D, shuffle = False)\n",
        "\n",
        "num_batches = X_test.shape[0] // (BATCH_SIZE*SEQUENCE_LENGTH_D)\n",
        "true = []\n",
        "ypreds = []\n",
        "\n",
        "for bx in range(num_batches):\n",
        "    x_batch, y_batch = next(test_batch_generator)\n",
        "    seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
        "    seq_len_d = []               \n",
        "    l = SEQUENCE_LENGTH_D\n",
        "    for i in range(0,len(x_batch),l):\n",
        "        for j in range(i,i+l):\n",
        "            if list(x_batch[j]).index(0) == 0:\n",
        "                seq_len_d.append(j%l)\n",
        "                break\n",
        "            elif j == i+l-1:\n",
        "                seq_len_d.append(l)\n",
        "\n",
        "    seq_len_d = np.array(seq_len_d)\n",
        "\n",
        "    y_preds_,loss_t,att_test = sess.run([y_preds_asap,loss_asap,attention_output_],\n",
        "                  feed_dict={batch_ph: x_batch,\n",
        "                        target_ph_asap: y_batch,\n",
        "                        seq_len_ph: seq_len,\n",
        "                        seq_len_ph_d: seq_len_d,\n",
        "                        doc_size_ph: doc_size_np,\n",
        "                        keep_prob_ph: 1.0})\n",
        "    ypreds.extend(y_preds_)\n",
        "    t = np.argmax(y_batch, axis = 1)\n",
        "    true.extend(t)\n",
        "\n",
        "true = true[:y_test_len]\n",
        "ypreds = ypreds[:y_test_len]\n",
        "\n",
        "spr = c(true, ypreds)\n",
        "\n",
        "if ordinal:\n",
        "    spr = spr[0]\n",
        "print('Test set '+ str_score + str(spr))\n",
        "\n",
        "rank = stats.spearmanr\n",
        "print('sp rho')\n",
        "print(rank(true, ypreds))\n",
        "\n",
        "from sklearn.metrics import cohen_kappa_score as kappa\n",
        "print('qwk')\n",
        "print(kappa(true, ypreds, weights=\"quadratic\"))\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "print('pearson')\n",
        "print(pearsonr(true,ypreds))\n",
        "\n",
        "print('kappa')\n",
        "print(kappa(true, ypreds, weights=None))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set Spearman rank:0.8139355823171375\n",
            "sp rho\n",
            "SpearmanrResult(correlation=0.8139355823171375, pvalue=9.68129220482734e-86)\n",
            "qwk\n",
            "0.765090909090909\n",
            "pearson\n",
            "(0.8324962498283603, 4.571985159788707e-93)\n",
            "kappa\n",
            "0.319013362825573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJfIQIAOPD0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}